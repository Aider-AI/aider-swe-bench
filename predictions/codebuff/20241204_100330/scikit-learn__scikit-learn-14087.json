{
    "instance_id": "scikit-learn__scikit-learn-14087",
    "model_name_or_path": "codebuff",
    "model_patch": "diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py\nindex 1999f1dae..b8d7f4e01 100644\n--- a/sklearn/linear_model/logistic.py\n+++ b/sklearn/linear_model/logistic.py\n@@ -1,3 +1,4 @@\n+```python\n \"\"\"\n Logistic Regression\n \"\"\"\n@@ -8,7 +9,7 @@ Logistic Regression\n #         Manoj Kumar <manojkumarsivaraj334@gmail.com>\n #         Lars Buitinck\n #         Simon Wu <s8wu@uwaterloo.ca>\n-#         Arthur Mensch <arthur.mensch@m4x.org\n+#         Arthur Mensch <arthur.mensch@m4x.org>\n \n import numbers\n import warnings\n@@ -1339,7 +1340,7 @@ class LogisticRegression(BaseEstimator, LinearClassifierMixin,\n \n     l1_ratio : float or None, optional (default=None)\n         The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n-        used if ``penalty='elasticnet'`. Setting ``l1_ratio=0`` is equivalent\n+        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n         to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n         to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n         combination of L1 and L2.\n@@ -1702,545 +1703,4 @@ class LogisticRegressionCV(LogisticRegression, BaseEstimator,\n \n     cv : int or cross-validation generator, optional (default=None)\n         The default cross-validation generator used is Stratified K-Folds.\n-        If an integer is provided, then it is the number of folds used.\n-        See the module :mod:`sklearn.model_selection` module for the\n-        list of possible cross-validation objects.\n-\n-        .. versionchanged:: 0.22\n-            ``cv`` default value if None changed from 3-fold to 5-fold.\n-\n-    dual : bool, optional (default=False)\n-        Dual or primal formulation. Dual formulation is only implemented for\n-        l2 penalty with liblinear solver. Prefer dual=False when\n-        n_samples > n_features.\n-\n-    penalty : str, 'l1', 'l2', or 'elasticnet', optional (default='l2')\n-        Used to specify the norm used in the penalization. The 'newton-cg',\n-        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n-        only supported by the 'saga' solver.\n-\n-    scoring : string, callable, or None, optional (default=None)\n-        A string (see model evaluation documentation) or\n-        a scorer callable object / function with signature\n-        ``scorer(estimator, X, y)``. For a list of scoring functions\n-        that can be used, look at :mod:`sklearn.metrics`. The\n-        default scoring option used is 'accuracy'.\n-\n-    solver : str, {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, \\\n-             optional (default='lbfgs')\n-\n-        Algorithm to use in the optimization problem.\n-\n-        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n-          'saga' are faster for large ones.\n-        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n-          handle multinomial loss; 'liblinear' is limited to one-versus-rest\n-          schemes.\n-        - 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas\n-          'liblinear' and 'saga' handle L1 penalty.\n-        - 'liblinear' might be slower in LogisticRegressionCV because it does\n-          not handle warm-starting.\n-\n-        Note that 'sag' and 'saga' fast convergence is only guaranteed on\n-        features with approximately the same scale. You can preprocess the data\n-        with a scaler from sklearn.preprocessing.\n-\n-        .. versionadded:: 0.17\n-           Stochastic Average Gradient descent solver.\n-        .. versionadded:: 0.19\n-           SAGA solver.\n-\n-    tol : float, optional (default=1e-4)\n-        Tolerance for stopping criteria.\n-\n-    max_iter : int, optional (default=100)\n-        Maximum number of iterations of the optimization algorithm.\n-\n-    class_weight : dict or 'balanced', optional (default=None)\n-        Weights associated with classes in the form ``{class_label: weight}``.\n-        If not given, all classes are supposed to have weight one.\n-\n-        The \"balanced\" mode uses the values of y to automatically adjust\n-        weights inversely proportional to class frequencies in the input data\n-        as ``n_samples / (n_classes * np.bincount(y))``.\n-\n-        Note that these weights will be multiplied with sample_weight (passed\n-        through the fit method) if sample_weight is specified.\n-\n-        .. versionadded:: 0.17\n-           class_weight == 'balanced'\n-\n-    n_jobs : int or None, optional (default=None)\n-        Number of CPU cores used during the cross-validation loop.\n-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n-        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n-        for more details.\n-\n-    verbose : int, optional (default=0)\n-        For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to any\n-        positive number for verbosity.\n-\n-    refit : bool, optional (default=True)\n-        If set to True, the scores are averaged across all folds, and the\n-        coefs and the C that corresponds to the best score is taken, and a\n-        final refit is done using these parameters.\n-        Otherwise the coefs, intercepts and C that correspond to the\n-        best scores across folds are averaged.\n-\n-    intercept_scaling : float, optional (default=1)\n-        Useful only when the solver 'liblinear' is used\n-        and self.fit_intercept is set to True. In this case, x becomes\n-        [x, self.intercept_scaling],\n-        i.e. a \"synthetic\" feature with constant value equal to\n-        intercept_scaling is appended to the instance vector.\n-        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n-\n-        Note! the synthetic feature weight is subject to l1/l2 regularization\n-        as all other features.\n-        To lessen the effect of regularization on synthetic feature weight\n-        (and therefore on the intercept) intercept_scaling has to be increased.\n-\n-    multi_class : {'ovr', 'multinomial', 'auto'}, default='auto'\n-        If the option chosen is 'ovr', then a binary problem is fit for each\n-        label. For 'multinomial' the loss minimised is the multinomial loss fit\n-        across the entire probability distribution, *even when the data is\n-        binary*. 'multinomial' is unavailable when solver='liblinear'.\n-        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n-        and otherwise selects 'multinomial'.\n-\n-        .. versionadded:: 0.18\n-           Stochastic Average Gradient descent solver for 'multinomial' case.\n-        .. versionchanged:: 0.22\n-            Default changed from 'ovr' to 'auto' in 0.22.\n-\n-    random_state : int, RandomState instance or None, optional (default=None)\n-        If int, random_state is the seed used by the random number generator;\n-        If RandomState instance, random_state is the random number generator;\n-        If None, the random number generator is the RandomState instance used\n-        by `np.random`.\n-\n-    l1_ratios : list of float or None, optional (default=None)\n-        The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.\n-        Only used if ``penalty='elasticnet'``. A value of 0 is equivalent to\n-        using ``penalty='l2'``, while 1 is equivalent to using\n-        ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combination\n-        of L1 and L2.\n-\n-    Attributes\n-    ----------\n-    classes_ : array, shape (n_classes, )\n-        A list of class labels known to the classifier.\n-\n-    coef_ : array, shape (1, n_features) or (n_classes, n_features)\n-        Coefficient of the features in the decision function.\n-\n-        `coef_` is of shape (1, n_features) when the given problem\n-        is binary.\n-\n-    intercept_ : array, shape (1,) or (n_classes,)\n-        Intercept (a.k.a. bias) added to the decision function.\n-\n-        If `fit_intercept` is set to False, the intercept is set to zero.\n-        `intercept_` is of shape(1,) when the problem is binary.\n-\n-    Cs_ : array, shape (n_cs)\n-        Array of C i.e. inverse of regularization parameter values used\n-        for cross-validation.\n-\n-    l1_ratios_ : array, shape (n_l1_ratios)\n-        Array of l1_ratios used for cross-validation. If no l1_ratio is used\n-        (i.e. penalty is not 'elasticnet'), this is set to ``[None]``\n-\n-    coefs_paths_ : array, shape (n_folds, n_cs, n_features) or \\\n-                   (n_folds, n_cs, n_features + 1)\n-        dict with classes as the keys, and the path of coefficients obtained\n-        during cross-validating across each fold and then across each Cs\n-        after doing an OvR for the corresponding class as values.\n-        If the 'multi_class' option is set to 'multinomial', then\n-        the coefs_paths are the coefficients corresponding to each class.\n-        Each dict value has shape ``(n_folds, n_cs, n_features)`` or\n-        ``(n_folds, n_cs, n_features + 1)`` depending on whether the\n-        intercept is fit or not. If ``penalty='elasticnet'``, the shape is\n-        ``(n_folds, n_cs, n_l1_ratios_, n_features)`` or\n-        ``(n_folds, n_cs, n_l1_ratios_, n_features + 1)``.\n-\n-    scores_ : dict\n-        dict with classes as the keys, and the values as the\n-        grid of scores obtained during cross-validating each fold, after doing\n-        an OvR for the corresponding class. If the 'multi_class' option\n-        given is 'multinomial' then the same scores are repeated across\n-        all classes, since this is the multinomial class. Each dict value\n-        has shape ``(n_folds, n_cs`` or ``(n_folds, n_cs, n_l1_ratios)`` if\n-        ``penalty='elasticnet'``.\n-\n-    C_ : array, shape (n_classes,) or (n_classes - 1,)\n-        Array of C that maps to the best scores across every class. If refit is\n-        set to False, then for each class, the best C is the average of the\n-        C's that correspond to the best scores for each fold.\n-        `C_` is of shape(n_classes,) when the problem is binary.\n-\n-    l1_ratio_ : array, shape (n_classes,) or (n_classes - 1,)\n-        Array of l1_ratio that maps to the best scores across every class. If\n-        refit is set to False, then for each class, the best l1_ratio is the\n-        average of the l1_ratio's that correspond to the best scores for each\n-        fold.  `l1_ratio_` is of shape(n_classes,) when the problem is binary.\n-\n-    n_iter_ : array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)\n-        Actual number of iterations for all classes, folds and Cs.\n-        In the binary or multinomial cases, the first dimension is equal to 1.\n-        If ``penalty='elasticnet'``, the shape is ``(n_classes, n_folds,\n-        n_cs, n_l1_ratios)`` or ``(1, n_folds, n_cs, n_l1_ratios)``.\n-\n-\n-    Examples\n-    --------\n-    >>> from sklearn.datasets import load_iris\n-    >>> from sklearn.linear_model import LogisticRegressionCV\n-    >>> X, y = load_iris(return_X_y=True)\n-    >>> clf = LogisticRegressionCV(cv=5, random_state=0).fit(X, y)\n-    >>> clf.predict(X[:2, :])\n-    array([0, 0])\n-    >>> clf.predict_proba(X[:2, :]).shape\n-    (2, 3)\n-    >>> clf.score(X, y)\n-    0.98...\n-\n-    See also\n-    --------\n-    LogisticRegression\n-\n-    \"\"\"\n-    def __init__(self, Cs=10, fit_intercept=True, cv=None, dual=False,\n-                 penalty='l2', scoring=None, solver='lbfgs', tol=1e-4,\n-                 max_iter=100, class_weight=None, n_jobs=None, verbose=0,\n-                 refit=True, intercept_scaling=1., multi_class='auto',\n-                 random_state=None, l1_ratios=None):\n-        self.Cs = Cs\n-        self.fit_intercept = fit_intercept\n-        self.cv = cv\n-        self.dual = dual\n-        self.penalty = penalty\n-        self.scoring = scoring\n-        self.tol = tol\n-        self.max_iter = max_iter\n-        self.class_weight = class_weight\n-        self.n_jobs = n_jobs\n-        self.verbose = verbose\n-        self.solver = solver\n-        self.refit = refit\n-        self.intercept_scaling = intercept_scaling\n-        self.multi_class = multi_class\n-        self.random_state = random_state\n-        self.l1_ratios = l1_ratios\n-\n-    def fit(self, X, y, sample_weight=None):\n-        \"\"\"Fit the model according to the given training data.\n-\n-        Parameters\n-        ----------\n-        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n-            Training vector, where n_samples is the number of samples and\n-            n_features is the number of features.\n-\n-        y : array-like, shape (n_samples,)\n-            Target vector relative to X.\n-\n-        sample_weight : array-like, shape (n_samples,) optional\n-            Array of weights that are assigned to individual samples.\n-            If not provided, then each sample is given unit weight.\n-\n-        Returns\n-        -------\n-        self : object\n-        \"\"\"\n-        solver = _check_solver(self.solver, self.penalty, self.dual)\n-\n-        if not isinstance(self.max_iter, numbers.Number) or self.max_iter < 0:\n-            raise ValueError(\"Maximum number of iteration must be positive;\"\n-                             \" got (max_iter=%r)\" % self.max_iter)\n-        if not isinstance(self.tol, numbers.Number) or self.tol < 0:\n-            raise ValueError(\"Tolerance for stopping criteria must be \"\n-                             \"positive; got (tol=%r)\" % self.tol)\n-        if self.penalty == 'elasticnet':\n-            if self.l1_ratios is None or len(self.l1_ratios) == 0 or any(\n-                    (not isinstance(l1_ratio, numbers.Number) or l1_ratio < 0\n-                     or l1_ratio > 1) for l1_ratio in self.l1_ratios):\n-                raise ValueError(\"l1_ratios must be a list of numbers between \"\n-                                 \"0 and 1; got (l1_ratios=%r)\" %\n-                                 self.l1_ratios)\n-            l1_ratios_ = self.l1_ratios\n-        else:\n-            if self.l1_ratios is not None:\n-                warnings.warn(\"l1_ratios parameter is only used when penalty \"\n-                              \"is 'elasticnet'. Got (penalty={})\".format(\n-                                  self.penalty))\n-\n-            l1_ratios_ = [None]\n-\n-        if self.penalty == 'none':\n-            raise ValueError(\n-                \"penalty='none' is not useful and not supported by \"\n-                \"LogisticRegressionCV.\"\n-            )\n-\n-        X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n-                         order=\"C\",\n-                         accept_large_sparse=solver != 'liblinear')\n-        check_classification_targets(y)\n-\n-        class_weight = self.class_weight\n-\n-        # Encode for string labels\n-        label_encoder = LabelEncoder().fit(y)\n-        y = label_encoder.transform(y)\n-        if isinstance(class_weight, dict):\n-            class_weight = {label_encoder.transform([cls])[0]: v\n-                            for cls, v in class_weight.items()}\n-\n-        # The original class labels\n-        classes = self.classes_ = label_encoder.classes_\n-        encoded_labels = label_encoder.transform(label_encoder.classes_)\n-\n-        multi_class = _check_multi_class(self.multi_class, solver,\n-                                         len(classes))\n-\n-        if solver in ['sag', 'saga']:\n-            max_squared_sum = row_norms(X, squared=True).max()\n-        else:\n-            max_squared_sum = None\n-\n-        # init cross-validation generator\n-        cv = check_cv(self.cv, y, classifier=True)\n-        folds = list(cv.split(X, y))\n-\n-        # Use the label encoded classes\n-        n_classes = len(encoded_labels)\n-\n-        if n_classes < 2:\n-            raise ValueError(\"This solver needs samples of at least 2 classes\"\n-                             \" in the data, but the data contains only one\"\n-                             \" class: %r\" % classes[0])\n-\n-        if n_classes == 2:\n-            # OvR in case of binary problems is as good as fitting\n-            # the higher label\n-            n_classes = 1\n-            encoded_labels = encoded_labels[1:]\n-            classes = classes[1:]\n-\n-        # We need this hack to iterate only once over labels, in the case of\n-        # multi_class = multinomial, without changing the value of the labels.\n-        if multi_class == 'multinomial':\n-            iter_encoded_labels = iter_classes = [None]\n-        else:\n-            iter_encoded_labels = encoded_labels\n-            iter_classes = classes\n-\n-        # compute the class weights for the entire dataset y\n-        if class_weight == \"balanced\":\n-            class_weight = compute_class_weight(class_weight,\n-                                                np.arange(len(self.classes_)),\n-                                                y)\n-            class_weight = dict(enumerate(class_weight))\n-\n-        path_func = delayed(_log_reg_scoring_path)\n-\n-        # The SAG solver releases the GIL so it's more efficient to use\n-        # threads for this solver.\n-        if self.solver in ['sag', 'saga']:\n-            prefer = 'threads'\n-        else:\n-            prefer = 'processes'\n-\n-        fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n-                               **_joblib_parallel_args(prefer=prefer))(\n-            path_func(X, y, train, test, pos_class=label, Cs=self.Cs,\n-                      fit_intercept=self.fit_intercept, penalty=self.penalty,\n-                      dual=self.dual, solver=solver, tol=self.tol,\n-                      max_iter=self.max_iter, verbose=self.verbose,\n-                      class_weight=class_weight, scoring=self.scoring,\n-                      multi_class=multi_class,\n-                      intercept_scaling=self.intercept_scaling,\n-                      random_state=self.random_state,\n-                      max_squared_sum=max_squared_sum,\n-                      sample_weight=sample_weight,\n-                      l1_ratio=l1_ratio\n-                      )\n-            for label in iter_encoded_labels\n-            for train, test in folds\n-            for l1_ratio in l1_ratios_)\n-\n-        # _log_reg_scoring_path will output different shapes depending on the\n-        # multi_class param, so we need to reshape the outputs accordingly.\n-        # Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\n-        # rows are equal, so we just take the first one.\n-        # After reshaping,\n-        # - scores is of shape (n_classes, n_folds, n_Cs . n_l1_ratios)\n-        # - coefs_paths is of shape\n-        #  (n_classes, n_folds, n_Cs . n_l1_ratios, n_features)\n-        # - n_iter is of shape\n-        #  (n_classes, n_folds, n_Cs . n_l1_ratios) or\n-        #  (1, n_folds, n_Cs . n_l1_ratios)\n-        coefs_paths, Cs, scores, n_iter_ = zip(*fold_coefs_)\n-        self.Cs_ = Cs[0]\n-        if multi_class == 'multinomial':\n-            coefs_paths = np.reshape(\n-                coefs_paths,\n-                (len(folds),  len(l1_ratios_) * len(self.Cs_), n_classes, -1)\n-            )\n-            # equiv to coefs_paths = np.moveaxis(coefs_paths, (0, 1, 2, 3),\n-            #                                                 (1, 2, 0, 3))\n-            coefs_paths = np.swapaxes(coefs_paths, 0, 1)\n-            coefs_paths = np.swapaxes(coefs_paths, 0, 2)\n-            self.n_iter_ = np.reshape(\n-                n_iter_,\n-                (1, len(folds), len(self.Cs_) * len(l1_ratios_))\n-            )\n-            # repeat same scores across all classes\n-            scores = np.tile(scores, (n_classes, 1, 1))\n-        else:\n-            coefs_paths = np.reshape(\n-                coefs_paths,\n-                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_),\n-                 -1)\n-            )\n-            self.n_iter_ = np.reshape(\n-                n_iter_,\n-                (n_classes, len(folds), len(self.Cs_) * len(l1_ratios_))\n-            )\n-        scores = np.reshape(scores, (n_classes, len(folds), -1))\n-        self.scores_ = dict(zip(classes, scores))\n-        self.coefs_paths_ = dict(zip(classes, coefs_paths))\n-\n-        self.C_ = list()\n-        self.l1_ratio_ = list()\n-        self.coef_ = np.empty((n_classes, X.shape[1]))\n-        self.intercept_ = np.zeros(n_classes)\n-        for index, (cls, encoded_label) in enumerate(\n-                zip(iter_classes, iter_encoded_labels)):\n-\n-            if multi_class == 'ovr':\n-                scores = self.scores_[cls]\n-                coefs_paths = self.coefs_paths_[cls]\n-            else:\n-                # For multinomial, all scores are the same across classes\n-                scores = scores[0]\n-                # coefs_paths will keep its original shape because\n-                # logistic_regression_path expects it this way\n-\n-            if self.refit:\n-                # best_index is between 0 and (n_Cs . n_l1_ratios - 1)\n-                # for example, with n_cs=2 and n_l1_ratios=3\n-                # the layout of scores is\n-                # [c1, c2, c1, c2, c1, c2]\n-                #   l1_1 ,  l1_2 ,  l1_3\n-                best_index = scores.sum(axis=0).argmax()\n-\n-                best_index_C = best_index % len(self.Cs_)\n-                C_ = self.Cs_[best_index_C]\n-                self.C_.append(C_)\n-\n-                best_index_l1 = best_index // len(self.Cs_)\n-                l1_ratio_ = l1_ratios_[best_index_l1]\n-                self.l1_ratio_.append(l1_ratio_)\n-\n-                if multi_class == 'multinomial':\n-                    coef_init = np.mean(coefs_paths[:, :, best_index, :],\n-                                        axis=1)\n-                else:\n-                    coef_init = np.mean(coefs_paths[:, best_index, :], axis=0)\n-\n-                # Note that y is label encoded and hence pos_class must be\n-                # the encoded label / None (for 'multinomial')\n-                w, _, _ = _logistic_regression_path(\n-                    X, y, pos_class=encoded_label, Cs=[C_], solver=solver,\n-                    fit_intercept=self.fit_intercept, coef=coef_init,\n-                    max_iter=self.max_iter, tol=self.tol,\n-                    penalty=self.penalty,\n-                    class_weight=class_weight,\n-                    multi_class=multi_class,\n-                    verbose=max(0, self.verbose - 1),\n-                    random_state=self.random_state,\n-                    check_input=False, max_squared_sum=max_squared_sum,\n-                    sample_weight=sample_weight,\n-                    l1_ratio=l1_ratio_)\n-                w = w[0]\n-\n-            else:\n-                # Take the best scores across every fold and the average of\n-                # all coefficients corresponding to the best scores.\n-                best_indices = np.argmax(scores, axis=1)\n-                if self.multi_class == 'ovr':\n-                    w = np.mean([coefs_paths[i, best_indices[i], :]\n-                                 for i in range(len(folds))], axis=0)\n-                else:\n-                    w = np.mean([coefs_paths[:, i, best_indices[i], :]\n-                                 for i in range(len(folds))], axis=0)\n-\n-                best_indices_C = best_indices % len(self.Cs_)\n-                self.C_.append(np.mean(self.Cs_[best_indices_C]))\n-\n-                best_indices_l1 = best_indices // len(self.Cs_)\n-                self.l1_ratio_.append(np.mean(l1_ratios_[best_indices_l1]))\n-\n-            if multi_class == 'multinomial':\n-                self.C_ = np.tile(self.C_, n_classes)\n-                self.l1_ratio_ = np.tile(self.l1_ratio_, n_classes)\n-                self.coef_ = w[:, :X.shape[1]]\n-                if self.fit_intercept:\n-                    self.intercept_ = w[:, -1]\n-            else:\n-                self.coef_[index] = w[: X.shape[1]]\n-                if self.fit_intercept:\n-                    self.intercept_[index] = w[-1]\n-\n-        self.C_ = np.asarray(self.C_)\n-        self.l1_ratio_ = np.asarray(self.l1_ratio_)\n-        self.l1_ratios_ = np.asarray(l1_ratios_)\n-        # if elasticnet was used, add the l1_ratios dimension to some\n-        # attributes\n-        if self.l1_ratios is not None:\n-            for cls, coefs_path in self.coefs_paths_.items():\n-                self.coefs_paths_[cls] = coefs_path.reshape(\n-                    (len(folds), self.Cs_.size, self.l1_ratios_.size, -1))\n-            for cls, score in self.scores_.items():\n-                self.scores_[cls] = score.reshape(\n-                    (len(folds), self.Cs_.size, self.l1_ratios_.size))\n-            self.n_iter_ = self.n_iter_.reshape(\n-                (-1, len(folds), self.Cs_.size, self.l1_ratios_.size))\n-\n-        return self\n-\n-    def score(self, X, y, sample_weight=None):\n-        \"\"\"Returns the score using the `scoring` option on the given\n-        test data and labels.\n-\n-        Parameters\n-        ----------\n-        X : array-like, shape = (n_samples, n_features)\n-            Test samples.\n-\n-        y : array-like, shape = (n_samples,)\n-            True labels for X.\n-\n-        sample_weight : array-like, shape = [n_samples], optional\n-            Sample weights.\n-\n-        Returns\n-        -------\n-        score : float\n-            Score of self.predict(X) wrt. y.\n-\n-        \"\"\"\n-\n-        if self.scoring is not None:\n-            warnings.warn(\"The long-standing behavior to use the \"\n-                          \"accuracy score has changed. The scoring \"\n-                          \"parameter is now used. \"\n-                          \"This warning will disappear in version 0.22.\",\n-                          ChangedBehaviorWarning)\n-        scoring = self.scoring or 'accuracy'\n-        if isinstance(scoring, str):\n-            scoring = get_scorer(scoring)\n-\n-        return scoring(self, X, y, sample_weight=sample_weight)\n+        If an integer is\n",
    "cost": 0,
    "edit_outcome": true,
    "lint_outcome": true,
    "test_outcome": false,
    "resolved": false
}