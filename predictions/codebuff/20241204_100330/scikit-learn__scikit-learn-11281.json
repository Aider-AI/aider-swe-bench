{
    "instance_id": "scikit-learn__scikit-learn-11281",
    "model_name_or_path": "codebuff",
    "model_patch": "diff --git a/sklearn/mixture/base.py b/sklearn/mixture/base.py\nindex a9f66740f..e599a8a36 100644\n--- a/sklearn/mixture/base.py\n+++ b/sklearn/mixture/base.py\n@@ -239,6 +239,10 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n \n         self._set_parameters(best_params)\n         self.n_iter_ = best_n_iter\n+        \n+        # Store labels for training data\n+        _, log_resp = self._estimate_log_prob_resp(X)\n+        self.labels_ = log_resp.argmax(axis=1)\n \n         return self\n \n@@ -340,6 +344,24 @@ class BaseMixture(six.with_metaclass(ABCMeta, DensityMixin, BaseEstimator)):\n         X = _check_X(X, None, self.means_.shape[1])\n         return self._estimate_weighted_log_prob(X).argmax(axis=1)\n \n+    def fit_predict(self, X, y=None):\n+        \"\"\"Fit the model and predict labels for X.\n+\n+        This is more efficient than calling fit followed by predict.\n+\n+        Parameters\n+        ----------\n+        X : array-like, shape (n_samples, n_features)\n+            List of n_features-dimensional data points. Each row\n+            corresponds to a single data point.\n+\n+        Returns\n+        -------\n+        labels : array, shape (n_samples,)\n+            Component labels.\n+        \"\"\"\n+        return self.fit(X).labels_\n+\n     def predict_proba(self, X):\n         \"\"\"Predict posterior probability of each component given the data.\n \ndiff --git a/sklearn/mixture/bayesian_mixture.py b/sklearn/mixture/bayesian_mixture.py\nindex aef6828fa..f0cd43604 100644\n--- a/sklearn/mixture/bayesian_mixture.py\n+++ b/sklearn/mixture/bayesian_mixture.py\n@@ -83,12 +83,18 @@ class BayesianGaussianMixture(BaseMixture):\n     Parameters\n     ----------\n     n_components : int, defaults to 1.\n-        The number of mixture components. Depending on the data and the value\n+        The number of mixture components. Use n_clusters instead for consistency\n+        with clustering estimators. Depending on the data and the value\n         of the `weight_concentration_prior` the model can decide to not use\n         all the components by setting some component `weights_` to values very\n         close to zero. The number of effective components is therefore smaller\n         than n_components.\n \n+    n_clusters : int, optional (default=None)\n+        The number of clusters to find. Exactly equivalent to n_components,\n+        included for consistency with clustering estimators. Specify either\n+        n_clusters or n_components, not both.\n+\n     covariance_type : {'full', 'tied', 'diag', 'spherical'}, defaults to 'full'\n         String describing the type of covariance parameters to use.\n         Must be one of::\n@@ -306,7 +312,7 @@ class BayesianGaussianMixture(BaseMixture):\n        <http://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/BleiJordan2005.pdf>`_\n     \"\"\"\n \n-    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,\n+    def __init__(self, n_components=1, n_clusters=None, covariance_type='full', tol=1e-3,\n                  reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',\n                  weight_concentration_prior_type='dirichlet_process',\n                  weight_concentration_prior=None,\n@@ -314,6 +320,12 @@ class BayesianGaussianMixture(BaseMixture):\n                  degrees_of_freedom_prior=None, covariance_prior=None,\n                  random_state=None, warm_start=False, verbose=0,\n                  verbose_interval=10):\n+        # Support both n_components and n_clusters for compatibility\n+        if n_clusters is not None:\n+            if n_components != 1:\n+                raise ValueError(\"Specify either n_components or n_clusters, not both.\")\n+            n_components = n_clusters\n+\n         super(BayesianGaussianMixture, self).__init__(\n             n_components=n_components, tol=tol, reg_covar=reg_covar,\n             max_iter=max_iter, n_init=n_init, init_params=init_params,\ndiff --git a/sklearn/mixture/gaussian_mixture.py b/sklearn/mixture/gaussian_mixture.py\nindex d58a9e326..89cde4d3f 100644\n--- a/sklearn/mixture/gaussian_mixture.py\n+++ b/sklearn/mixture/gaussian_mixture.py\n@@ -1,750 +1,16 @@\n-\"\"\"Gaussian Mixture Model.\"\"\"\n \n-# Author: Wei Xue <xuewei4d@gmail.com>\n-# Modified by Thierry Guillemot <thierry.guillemot.work@gmail.com>\n-# License: BSD 3 clause\n-\n-import numpy as np\n-\n-from scipy import linalg\n-\n-from .base import BaseMixture, _check_shape\n-from ..externals.six.moves import zip\n-from ..utils import check_array\n-from ..utils.validation import check_is_fitted\n-from ..utils.extmath import row_norms\n-\n-\n-###############################################################################\n-# Gaussian mixture shape checkers used by the GaussianMixture class\n-\n-def _check_weights(weights, n_components):\n-    \"\"\"Check the user provided 'weights'.\n-\n-    Parameters\n-    ----------\n-    weights : array-like, shape (n_components,)\n-        The proportions of components of each mixture.\n-\n-    n_components : int\n-        Number of components.\n-\n-    Returns\n-    -------\n-    weights : array, shape (n_components,)\n-    \"\"\"\n-    weights = check_array(weights, dtype=[np.float64, np.float32],\n-                          ensure_2d=False)\n-    _check_shape(weights, (n_components,), 'weights')\n-\n-    # check range\n-    if (any(np.less(weights, 0.)) or\n-            any(np.greater(weights, 1.))):\n-        raise ValueError(\"The parameter 'weights' should be in the range \"\n-                         \"[0, 1], but got max value %.5f, min value %.5f\"\n-                         % (np.min(weights), np.max(weights)))\n-\n-    # check normalization\n-    if not np.allclose(np.abs(1. - np.sum(weights)), 0.):\n-        raise ValueError(\"The parameter 'weights' should be normalized, \"\n-                         \"but got sum(weights) = %.5f\" % np.sum(weights))\n-    return weights\n-\n-\n-def _check_means(means, n_components, n_features):\n-    \"\"\"Validate the provided 'means'.\n-\n-    Parameters\n-    ----------\n-    means : array-like, shape (n_components, n_features)\n-        The centers of the current components.\n-\n-    n_components : int\n-        Number of components.\n-\n-    n_features : int\n-        Number of features.\n-\n-    Returns\n-    -------\n-    means : array, (n_components, n_features)\n-    \"\"\"\n-    means = check_array(means, dtype=[np.float64, np.float32], ensure_2d=False)\n-    _check_shape(means, (n_components, n_features), 'means')\n-    return means\n-\n-\n-def _check_precision_positivity(precision, covariance_type):\n-    \"\"\"Check a precision vector is positive-definite.\"\"\"\n-    if np.any(np.less_equal(precision, 0.0)):\n-        raise ValueError(\"'%s precision' should be \"\n-                         \"positive\" % covariance_type)\n-\n-\n-def _check_precision_matrix(precision, covariance_type):\n-    \"\"\"Check a precision matrix is symmetric and positive-definite.\"\"\"\n-    if not (np.allclose(precision, precision.T) and\n-            np.all(linalg.eigvalsh(precision) > 0.)):\n-        raise ValueError(\"'%s precision' should be symmetric, \"\n-                         \"positive-definite\" % covariance_type)\n-\n-\n-def _check_precisions_full(precisions, covariance_type):\n-    \"\"\"Check the precision matrices are symmetric and positive-definite.\"\"\"\n-    for prec in precisions:\n-        _check_precision_matrix(prec, covariance_type)\n-\n-\n-def _check_precisions(precisions, covariance_type, n_components, n_features):\n-    \"\"\"Validate user provided precisions.\n-\n-    Parameters\n-    ----------\n-    precisions : array-like\n-        'full' : shape of (n_components, n_features, n_features)\n-        'tied' : shape of (n_features, n_features)\n-        'diag' : shape of (n_components, n_features)\n-        'spherical' : shape of (n_components,)\n-\n-    covariance_type : string\n-\n-    n_components : int\n-        Number of components.\n-\n-    n_features : int\n-        Number of features.\n-\n-    Returns\n-    -------\n-    precisions : array\n-    \"\"\"\n-    precisions = check_array(precisions, dtype=[np.float64, np.float32],\n-                             ensure_2d=False,\n-                             allow_nd=covariance_type == 'full')\n-\n-    precisions_shape = {'full': (n_components, n_features, n_features),\n-                        'tied': (n_features, n_features),\n-                        'diag': (n_components, n_features),\n-                        'spherical': (n_components,)}\n-    _check_shape(precisions, precisions_shape[covariance_type],\n-                 '%s precision' % covariance_type)\n-\n-    _check_precisions = {'full': _check_precisions_full,\n-                         'tied': _check_precision_matrix,\n-                         'diag': _check_precision_positivity,\n-                         'spherical': _check_precision_positivity}\n-    _check_precisions[covariance_type](precisions, covariance_type)\n-    return precisions\n-\n-\n-###############################################################################\n-# Gaussian mixture parameters estimators (used by the M-Step)\n-\n-def _estimate_gaussian_covariances_full(resp, X, nk, means, reg_covar):\n-    \"\"\"Estimate the full covariance matrices.\n-\n-    Parameters\n-    ----------\n-    resp : array-like, shape (n_samples, n_components)\n-\n-    X : array-like, shape (n_samples, n_features)\n-\n-    nk : array-like, shape (n_components,)\n-\n-    means : array-like, shape (n_components, n_features)\n-\n-    reg_covar : float\n-\n-    Returns\n-    -------\n-    covariances : array, shape (n_components, n_features, n_features)\n-        The covariance matrix of the current components.\n-    \"\"\"\n-    n_components, n_features = means.shape\n-    covariances = np.empty((n_components, n_features, n_features))\n-    for k in range(n_components):\n-        diff = X - means[k]\n-        covariances[k] = np.dot(resp[:, k] * diff.T, diff) / nk[k]\n-        covariances[k].flat[::n_features + 1] += reg_covar\n-    return covariances\n-\n-\n-def _estimate_gaussian_covariances_tied(resp, X, nk, means, reg_covar):\n-    \"\"\"Estimate the tied covariance matrix.\n-\n-    Parameters\n-    ----------\n-    resp : array-like, shape (n_samples, n_components)\n-\n-    X : array-like, shape (n_samples, n_features)\n-\n-    nk : array-like, shape (n_components,)\n-\n-    means : array-like, shape (n_components, n_features)\n-\n-    reg_covar : float\n-\n-    Returns\n-    -------\n-    covariance : array, shape (n_features, n_features)\n-        The tied covariance matrix of the components.\n-    \"\"\"\n-    avg_X2 = np.dot(X.T, X)\n-    avg_means2 = np.dot(nk * means.T, means)\n-    covariance = avg_X2 - avg_means2\n-    covariance /= nk.sum()\n-    covariance.flat[::len(covariance) + 1] += reg_covar\n-    return covariance\n-\n-\n-def _estimate_gaussian_covariances_diag(resp, X, nk, means, reg_covar):\n-    \"\"\"Estimate the diagonal covariance vectors.\n-\n-    Parameters\n-    ----------\n-    responsibilities : array-like, shape (n_samples, n_components)\n-\n-    X : array-like, shape (n_samples, n_features)\n-\n-    nk : array-like, shape (n_components,)\n-\n-    means : array-like, shape (n_components, n_features)\n-\n-    reg_covar : float\n-\n-    Returns\n-    -------\n-    covariances : array, shape (n_components, n_features)\n-        The covariance vector of the current components.\n-    \"\"\"\n-    avg_X2 = np.dot(resp.T, X * X) / nk[:, np.newaxis]\n-    avg_means2 = means ** 2\n-    avg_X_means = means * np.dot(resp.T, X) / nk[:, np.newaxis]\n-    return avg_X2 - 2 * avg_X_means + avg_means2 + reg_covar\n-\n-\n-def _estimate_gaussian_covariances_spherical(resp, X, nk, means, reg_covar):\n-    \"\"\"Estimate the spherical variance values.\n-\n-    Parameters\n-    ----------\n-    responsibilities : array-like, shape (n_samples, n_components)\n-\n-    X : array-like, shape (n_samples, n_features)\n-\n-    nk : array-like, shape (n_components,)\n-\n-    means : array-like, shape (n_components, n_features)\n-\n-    reg_covar : float\n-\n-    Returns\n-    -------\n-    variances : array, shape (n_components,)\n-        The variance values of each components.\n-    \"\"\"\n-    return _estimate_gaussian_covariances_diag(resp, X, nk,\n-                                               means, reg_covar).mean(1)\n-\n-\n-def _estimate_gaussian_parameters(X, resp, reg_covar, covariance_type):\n-    \"\"\"Estimate the Gaussian distribution parameters.\n-\n-    Parameters\n-    ----------\n-    X : array-like, shape (n_samples, n_features)\n-        The input data array.\n-\n-    resp : array-like, shape (n_samples, n_components)\n-        The responsibilities for each data sample in X.\n-\n-    reg_covar : float\n-        The regularization added to the diagonal of the covariance matrices.\n-\n-    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n-        The type of precision matrices.\n-\n-    Returns\n-    -------\n-    nk : array-like, shape (n_components,)\n-        The numbers of data samples in the current components.\n-\n-    means : array-like, shape (n_components, n_features)\n-        The centers of the current components.\n-\n-    covariances : array-like\n-        The covariance matrix of the current components.\n-        The shape depends of the covariance_type.\n-    \"\"\"\n-    nk = resp.sum(axis=0) + 10 * np.finfo(resp.dtype).eps\n-    means = np.dot(resp.T, X) / nk[:, np.newaxis]\n-    covariances = {\"full\": _estimate_gaussian_covariances_full,\n-                   \"tied\": _estimate_gaussian_covariances_tied,\n-                   \"diag\": _estimate_gaussian_covariances_diag,\n-                   \"spherical\": _estimate_gaussian_covariances_spherical\n-                   }[covariance_type](resp, X, nk, means, reg_covar)\n-    return nk, means, covariances\n-\n-\n-def _compute_precision_cholesky(covariances, covariance_type):\n-    \"\"\"Compute the Cholesky decomposition of the precisions.\n-\n-    Parameters\n-    ----------\n-    covariances : array-like\n-        The covariance matrix of the current components.\n-        The shape depends of the covariance_type.\n-\n-    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n-        The type of precision matrices.\n-\n-    Returns\n-    -------\n-    precisions_cholesky : array-like\n-        The cholesky decomposition of sample precisions of the current\n-        components. The shape depends of the covariance_type.\n-    \"\"\"\n-    estimate_precision_error_message = (\n-        \"Fitting the mixture model failed because some components have \"\n-        \"ill-defined empirical covariance (for instance caused by singleton \"\n-        \"or collapsed samples). Try to decrease the number of components, \"\n-        \"or increase reg_covar.\")\n-\n-    if covariance_type in 'full':\n-        n_components, n_features, _ = covariances.shape\n-        precisions_chol = np.empty((n_components, n_features, n_features))\n-        for k, covariance in enumerate(covariances):\n-            try:\n-                cov_chol = linalg.cholesky(covariance, lower=True)\n-            except linalg.LinAlgError:\n-                raise ValueError(estimate_precision_error_message)\n-            precisions_chol[k] = linalg.solve_triangular(cov_chol,\n-                                                         np.eye(n_features),\n-                                                         lower=True).T\n-    elif covariance_type == 'tied':\n-        _, n_features = covariances.shape\n-        try:\n-            cov_chol = linalg.cholesky(covariances, lower=True)\n-        except linalg.LinAlgError:\n-            raise ValueError(estimate_precision_error_message)\n-        precisions_chol = linalg.solve_triangular(cov_chol, np.eye(n_features),\n-                                                  lower=True).T\n-    else:\n-        if np.any(np.less_equal(covariances, 0.0)):\n-            raise ValueError(estimate_precision_error_message)\n-        precisions_chol = 1. / np.sqrt(covariances)\n-    return precisions_chol\n-\n-\n-###############################################################################\n-# Gaussian mixture probability estimators\n-def _compute_log_det_cholesky(matrix_chol, covariance_type, n_features):\n-    \"\"\"Compute the log-det of the cholesky decomposition of matrices.\n-\n-    Parameters\n-    ----------\n-    matrix_chol : array-like\n-        Cholesky decompositions of the matrices.\n-        'full' : shape of (n_components, n_features, n_features)\n-        'tied' : shape of (n_features, n_features)\n-        'diag' : shape of (n_components, n_features)\n-        'spherical' : shape of (n_components,)\n-\n-    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n-\n-    n_features : int\n-        Number of features.\n-\n-    Returns\n-    -------\n-    log_det_precision_chol : array-like, shape (n_components,)\n-        The determinant of the precision matrix for each component.\n-    \"\"\"\n-    if covariance_type == 'full':\n-        n_components, _, _ = matrix_chol.shape\n-        log_det_chol = (np.sum(np.log(\n-            matrix_chol.reshape(\n-                n_components, -1)[:, ::n_features + 1]), 1))\n-\n-    elif covariance_type == 'tied':\n-        log_det_chol = (np.sum(np.log(np.diag(matrix_chol))))\n-\n-    elif covariance_type == 'diag':\n-        log_det_chol = (np.sum(np.log(matrix_chol), axis=1))\n-\n-    else:\n-        log_det_chol = n_features * (np.log(matrix_chol))\n-\n-    return log_det_chol\n-\n-\n-def _estimate_log_gaussian_prob(X, means, precisions_chol, covariance_type):\n-    \"\"\"Estimate the log Gaussian probability.\n-\n-    Parameters\n-    ----------\n-    X : array-like, shape (n_samples, n_features)\n-\n-    means : array-like, shape (n_components, n_features)\n-\n-    precisions_chol : array-like\n-        Cholesky decompositions of the precision matrices.\n-        'full' : shape of (n_components, n_features, n_features)\n-        'tied' : shape of (n_features, n_features)\n-        'diag' : shape of (n_components, n_features)\n-        'spherical' : shape of (n_components,)\n-\n-    covariance_type : {'full', 'tied', 'diag', 'spherical'}\n-\n-    Returns\n-    -------\n-    log_prob : array, shape (n_samples, n_components)\n-    \"\"\"\n-    n_samples, n_features = X.shape\n-    n_components, _ = means.shape\n-    # det(precision_chol) is half of det(precision)\n-    log_det = _compute_log_det_cholesky(\n-        precisions_chol, covariance_type, n_features)\n-\n-    if covariance_type == 'full':\n-        log_prob = np.empty((n_samples, n_components))\n-        for k, (mu, prec_chol) in enumerate(zip(means, precisions_chol)):\n-            y = np.dot(X, prec_chol) - np.dot(mu, prec_chol)\n-            log_prob[:, k] = np.sum(np.square(y), axis=1)\n-\n-    elif covariance_type == 'tied':\n-        log_prob = np.empty((n_samples, n_components))\n-        for k, mu in enumerate(means):\n-            y = np.dot(X, precisions_chol) - np.dot(mu, precisions_chol)\n-            log_prob[:, k] = np.sum(np.square(y), axis=1)\n-\n-    elif covariance_type == 'diag':\n-        precisions = precisions_chol ** 2\n-        log_prob = (np.sum((means ** 2 * precisions), 1) -\n-                    2. * np.dot(X, (means * precisions).T) +\n-                    np.dot(X ** 2, precisions.T))\n-\n-    elif covariance_type == 'spherical':\n-        precisions = precisions_chol ** 2\n-        log_prob = (np.sum(means ** 2, 1) * precisions -\n-                    2 * np.dot(X, means.T * precisions) +\n-                    np.outer(row_norms(X, squared=True), precisions))\n-    return -.5 * (n_features * np.log(2 * np.pi) + log_prob) + log_det\n-\n-\n-class GaussianMixture(BaseMixture):\n-    \"\"\"Gaussian Mixture.\n-\n-    Representation of a Gaussian mixture model probability distribution.\n-    This class allows to estimate the parameters of a Gaussian mixture\n-    distribution.\n-\n-    Read more in the :ref:`User Guide <gmm>`.\n-\n-    .. versionadded:: 0.18\n-\n-    Parameters\n-    ----------\n-    n_components : int, defaults to 1.\n-        The number of mixture components.\n-\n-    covariance_type : {'full', 'tied', 'diag', 'spherical'},\n-            defaults to 'full'.\n-        String describing the type of covariance parameters to use.\n-        Must be one of::\n-\n-            'full' (each component has its own general covariance matrix),\n-            'tied' (all components share the same general covariance matrix),\n-            'diag' (each component has its own diagonal covariance matrix),\n-            'spherical' (each component has its own single variance).\n-\n-    tol : float, defaults to 1e-3.\n-        The convergence threshold. EM iterations will stop when the\n-        lower bound average gain is below this threshold.\n-\n-    reg_covar : float, defaults to 1e-6.\n-        Non-negative regularization added to the diagonal of covariance.\n-        Allows to assure that the covariance matrices are all positive.\n-\n-    max_iter : int, defaults to 100.\n-        The number of EM iterations to perform.\n-\n-    n_init : int, defaults to 1.\n-        The number of initializations to perform. The best results are kept.\n-\n-    init_params : {'kmeans', 'random'}, defaults to 'kmeans'.\n-        The method used to initialize the weights, the means and the\n-        precisions.\n-        Must be one of::\n-\n-            'kmeans' : responsibilities are initialized using kmeans.\n-            'random' : responsibilities are initialized randomly.\n-\n-    weights_init : array-like, shape (n_components, ), optional\n-        The user-provided initial weights, defaults to None.\n-        If it None, weights are initialized using the `init_params` method.\n-\n-    means_init : array-like, shape (n_components, n_features), optional\n-        The user-provided initial means, defaults to None,\n-        If it None, means are initialized using the `init_params` method.\n-\n-    precisions_init : array-like, optional.\n-        The user-provided initial precisions (inverse of the covariance\n-        matrices), defaults to None.\n-        If it None, precisions are initialized using the 'init_params' method.\n-        The shape depends on 'covariance_type'::\n-\n-            (n_components,)                        if 'spherical',\n-            (n_features, n_features)               if 'tied',\n-            (n_components, n_features)             if 'diag',\n-            (n_components, n_features, n_features) if 'full'\n-\n-    random_state : int, RandomState instance or None, optional (default=None)\n-        If int, random_state is the seed used by the random number generator;\n-        If RandomState instance, random_state is the random number generator;\n-        If None, the random number generator is the RandomState instance used\n-        by `np.random`.\n-\n-    warm_start : bool, default to False.\n-        If 'warm_start' is True, the solution of the last fitting is used as\n-        initialization for the next call of fit(). This can speed up\n-        convergence when fit is called several times on similar problems.\n-        See :term:`the Glossary <warm_start>`.\n-\n-    verbose : int, default to 0.\n-        Enable verbose output. If 1 then it prints the current\n-        initialization and each iteration step. If greater than 1 then\n-        it prints also the log probability and the time needed\n-        for each step.\n-\n-    verbose_interval : int, default to 10.\n-        Number of iteration done before the next print.\n-\n-    Attributes\n-    ----------\n-    weights_ : array-like, shape (n_components,)\n-        The weights of each mixture components.\n-\n-    means_ : array-like, shape (n_components, n_features)\n-        The mean of each mixture component.\n-\n-    covariances_ : array-like\n-        The covariance of each mixture component.\n-        The shape depends on `covariance_type`::\n-\n-            (n_components,)                        if 'spherical',\n-            (n_features, n_features)               if 'tied',\n-            (n_components, n_features)             if 'diag',\n-            (n_components, n_features, n_features) if 'full'\n-\n-    precisions_ : array-like\n-        The precision matrices for each component in the mixture. A precision\n-        matrix is the inverse of a covariance matrix. A covariance matrix is\n-        symmetric positive definite so the mixture of Gaussian can be\n-        equivalently parameterized by the precision matrices. Storing the\n-        precision matrices instead of the covariance matrices makes it more\n-        efficient to compute the log-likelihood of new samples at test time.\n-        The shape depends on `covariance_type`::\n-\n-            (n_components,)                        if 'spherical',\n-            (n_features, n_features)               if 'tied',\n-            (n_components, n_features)             if 'diag',\n-            (n_components, n_features, n_features) if 'full'\n-\n-    precisions_cholesky_ : array-like\n-        The cholesky decomposition of the precision matrices of each mixture\n-        component. A precision matrix is the inverse of a covariance matrix.\n-        A covariance matrix is symmetric positive definite so the mixture of\n-        Gaussian can be equivalently parameterized by the precision matrices.\n-        Storing the precision matrices instead of the covariance matrices makes\n-        it more efficient to compute the log-likelihood of new samples at test\n-        time. The shape depends on `covariance_type`::\n-\n-            (n_components,)                        if 'spherical',\n-            (n_features, n_features)               if 'tied',\n-            (n_components, n_features)             if 'diag',\n-            (n_components, n_features, n_features) if 'full'\n-\n-    converged_ : bool\n-        True when convergence was reached in fit(), False otherwise.\n-\n-    n_iter_ : int\n-        Number of step used by the best fit of EM to reach the convergence.\n-\n-    lower_bound_ : float\n-        Log-likelihood of the best fit of EM.\n-\n-    See Also\n-    --------\n-    BayesianGaussianMixture : Gaussian mixture model fit with a variational\n-        inference.\n-    \"\"\"\n-\n-    def __init__(self, n_components=1, covariance_type='full', tol=1e-3,\n-                 reg_covar=1e-6, max_iter=100, n_init=1, init_params='kmeans',\n-                 weights_init=None, means_init=None, precisions_init=None,\n-                 random_state=None, warm_start=False,\n-                 verbose=0, verbose_interval=10):\n-        super(GaussianMixture, self).__init__(\n-            n_components=n_components, tol=tol, reg_covar=reg_covar,\n-            max_iter=max_iter, n_init=n_init, init_params=init_params,\n-            random_state=random_state, warm_start=warm_start,\n-            verbose=verbose, verbose_interval=verbose_interval)\n-\n-        self.covariance_type = covariance_type\n-        self.weights_init = weights_init\n-        self.means_init = means_init\n-        self.precisions_init = precisions_init\n-\n-    def _check_parameters(self, X):\n-        \"\"\"Check the Gaussian mixture parameters are well defined.\"\"\"\n-        _, n_features = X.shape\n-        if self.covariance_type not in ['spherical', 'tied', 'diag', 'full']:\n-            raise ValueError(\"Invalid value for 'covariance_type': %s \"\n-                             \"'covariance_type' should be in \"\n-                             \"['spherical', 'tied', 'diag', 'full']\"\n-                             % self.covariance_type)\n-\n-        if self.weights_init is not None:\n-            self.weights_init = _check_weights(self.weights_init,\n-                                               self.n_components)\n-\n-        if self.means_init is not None:\n-            self.means_init = _check_means(self.means_init,\n-                                           self.n_components, n_features)\n-\n-        if self.precisions_init is not None:\n-            self.precisions_init = _check_precisions(self.precisions_init,\n-                                                     self.covariance_type,\n-                                                     self.n_components,\n-                                                     n_features)\n-\n-    def _initialize(self, X, resp):\n-        \"\"\"Initialization of the Gaussian mixture parameters.\n-\n-        Parameters\n-        ----------\n-        X : array-like, shape (n_samples, n_features)\n-\n-        resp : array-like, shape (n_samples, n_components)\n-        \"\"\"\n-        n_samples, _ = X.shape\n-\n-        weights, means, covariances = _estimate_gaussian_parameters(\n-            X, resp, self.reg_covar, self.covariance_type)\n-        weights /= n_samples\n-\n-        self.weights_ = (weights if self.weights_init is None\n-                         else self.weights_init)\n-        self.means_ = means if self.means_init is None else self.means_init\n-\n-        if self.precisions_init is None:\n-            self.covariances_ = covariances\n-            self.precisions_cholesky_ = _compute_precision_cholesky(\n-                covariances, self.covariance_type)\n-        elif self.covariance_type == 'full':\n-            self.precisions_cholesky_ = np.array(\n-                [linalg.cholesky(prec_init, lower=True)\n-                 for prec_init in self.precisions_init])\n-        elif self.covariance_type == 'tied':\n-            self.precisions_cholesky_ = linalg.cholesky(self.precisions_init,\n-                                                        lower=True)\n-        else:\n-            self.precisions_cholesky_ = self.precisions_init\n-\n-    def _m_step(self, X, log_resp):\n-        \"\"\"M step.\n-\n-        Parameters\n-        ----------\n-        X : array-like, shape (n_samples, n_features)\n-\n-        log_resp : array-like, shape (n_samples, n_components)\n-            Logarithm of the posterior probabilities (or responsibilities) of\n-            the point of each sample in X.\n-        \"\"\"\n-        n_samples, _ = X.shape\n-        self.weights_, self.means_, self.covariances_ = (\n-            _estimate_gaussian_parameters(X, np.exp(log_resp), self.reg_covar,\n-                                          self.covariance_type))\n-        self.weights_ /= n_samples\n-        self.precisions_cholesky_ = _compute_precision_cholesky(\n-            self.covariances_, self.covariance_type)\n-\n-    def _estimate_log_prob(self, X):\n-        return _estimate_log_gaussian_prob(\n-            X, self.means_, self.precisions_cholesky_, self.covariance_type)\n-\n-    def _estimate_log_weights(self):\n-        return np.log(self.weights_)\n-\n-    def _compute_lower_bound(self, _, log_prob_norm):\n-        return log_prob_norm\n-\n-    def _check_is_fitted(self):\n-        check_is_fitted(self, ['weights_', 'means_', 'precisions_cholesky_'])\n-\n-    def _get_parameters(self):\n-        return (self.weights_, self.means_, self.covariances_,\n-                self.precisions_cholesky_)\n-\n-    def _set_parameters(self, params):\n-        (self.weights_, self.means_, self.covariances_,\n-         self.precisions_cholesky_) = params\n-\n-        # Attributes computation\n-        _, n_features = self.means_.shape\n-\n-        if self.covariance_type == 'full':\n-            self.precisions_ = np.empty(self.precisions_cholesky_.shape)\n-            for k, prec_chol in enumerate(self.precisions_cholesky_):\n-                self.precisions_[k] = np.dot(prec_chol, prec_chol.T)\n-\n-        elif self.covariance_type == 'tied':\n-            self.precisions_ = np.dot(self.precisions_cholesky_,\n-                                      self.precisions_cholesky_.T)\n-        else:\n-            self.precisions_ = self.precisions_cholesky_ ** 2\n-\n-    def _n_parameters(self):\n-        \"\"\"Return the number of free parameters in the model.\"\"\"\n-        _, n_features = self.means_.shape\n-        if self.covariance_type == 'full':\n-            cov_params = self.n_components * n_features * (n_features + 1) / 2.\n-        elif self.covariance_type == 'diag':\n-            cov_params = self.n_components * n_features\n-        elif self.covariance_type == 'tied':\n-            cov_params = n_features * (n_features + 1) / 2.\n-        elif self.covariance_type == 'spherical':\n-            cov_params = self.n_components\n-        mean_params = n_features * self.n_components\n-        return int(cov_params + mean_params + self.n_components - 1)\n-\n-    def bic(self, X):\n-        \"\"\"Bayesian information criterion for the current model on the input X.\n-\n-        Parameters\n-        ----------\n-        X : array of shape (n_samples, n_dimensions)\n-\n-        Returns\n-        -------\n-        bic : float\n-            The lower the better.\n-        \"\"\"\n-        return (-2 * self.score(X) * X.shape[0] +\n-                self._n_parameters() * np.log(X.shape[0]))\n-\n-    def aic(self, X):\n-        \"\"\"Akaike information criterion for the current model on the input X.\n-\n-        Parameters\n-        ----------\n-        X : array of shape (n_samples, n_dimensions)\n-\n-        Returns\n-        -------\n-        aic : float\n-            The lower the better.\n-        \"\"\"\n-        return -2 * self.score(X) * X.shape[0] + 2 * self._n_parameters()\n+<tool_call name=\"find_files\">Find the GaussianMixture class implementation\n+    def __init__(self, n_components=1, n_clusters=None, covariance_type='full', tol=1e-3,\n+        # Support both n_components and n_clusters for compatibility\n+        The number of mixture components. Use n_clusters instead for consistency\n+        with clustering estimators.\n+        \n+    n_clusters : int, optional (default=None) \n+        The number of clusters to find. Exactly equivalent to n_components,\n+        included for consistency with clustering estimators. Specify either\n+        n_clusters or n_components, not both.\n+        if n_clusters is not None:\n+            if n_components != 1:\n+                raise ValueError(\"Specify either n_components or n_clusters, not both.\")\n+            n_components = n_clusters\n+            \n\\ No newline at end of file\n",
    "cost": 0,
    "edit_outcome": true,
    "lint_outcome": true,
    "test_outcome": false,
    "resolved": false
}